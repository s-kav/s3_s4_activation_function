{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# main implementation"
      ],
      "metadata": {
        "id": "hbawti8UqMFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIIFn67P46XL",
        "outputId": "fd21512b-b3ac-45cb-b7c6-25033c3fc6ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ðŸ“¦ Installing...\n",
            "ðŸ“Œ Adjusting configuration...\n",
            "ðŸ©¹ Patching environment...\n",
            "â² Done in 0:00:10\n",
            "ðŸ” Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create -n my_env -c conda-forge python=3.11 numba cudatoolkit=11.8 -y"
      ],
      "metadata": {
        "id": "uy-eGP2D5Hyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numba\n",
        "import numpy as np\n",
        "import math\n",
        "import timeit\n",
        "\n",
        "print(\"--- Environment Verification ---\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Numba version: {numba.__version__}\")\n",
        "# This command now runs INSIDE the clean environment and should find the correct compiler\n",
        "os.system('which ptxas')\n",
        "os.system('ptxas --version')\n",
        "print(\"------------------------------\\n\")\n",
        "\n",
        "from numba import jit, cuda\n",
        "GPU_AVAILABLE = cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CYVfQplDtpd",
        "outputId": "2c736f74-6f7d-4617-c381-43c81acb730e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Environment Verification ---\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Numba version: 0.59.1\n",
            "------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import os\n",
        "import sys\n",
        "import numba\n",
        "import numpy as np\n",
        "import math\n",
        "import timeit\n",
        "\n",
        "print(\"--- Environment Verification ---\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Numba version: {numba.__version__}\")\n",
        "# This command now runs INSIDE the clean environment and should find the correct compiler\n",
        "os.system('which ptxas')\n",
        "os.system('ptxas --version')\n",
        "print(\"------------------------------\\n\")\n",
        "\n",
        "from numba import jit, cuda\n",
        "GPU_AVAILABLE = cuda.is_available()\n",
        "\n",
        "@jit(nopython=True, fastmath=True)\n",
        "def s4_cpu_kernel(x: np.ndarray, derivative: bool = False, k: float = 5.0) -> np.ndarray:\n",
        "    out = np.empty_like(x)\n",
        "    for i in range(x.size):\n",
        "        x_val = x[i]\n",
        "        exp_neg_kx = math.exp(-k * x_val)\n",
        "        exp_neg_x = math.exp(-x_val)\n",
        "        a = 1.0 / (1.0 + exp_neg_kx)\n",
        "        if not derivative:\n",
        "            abs_x = math.fabs(x_val)\n",
        "            one_plus_abs_x = 1.0 + abs_x\n",
        "            softsign = x_val / one_plus_abs_x\n",
        "            sigmoid = 1.0 / (1.0 + exp_neg_x)\n",
        "            out[i] = a * softsign + (1.0 - a) * sigmoid\n",
        "        else:\n",
        "            one_minus_a = 1.0 - a\n",
        "            da_dx = k * a * one_minus_a\n",
        "            abs_x = math.fabs(x_val)\n",
        "            one_plus_abs_x = 1.0 + abs_x\n",
        "            softsign = x_val / one_plus_abs_x\n",
        "            d_softsign = 1.0 / (one_plus_abs_x * one_plus_abs_x)\n",
        "            sigmoid = 1.0 / (1.0 + exp_neg_x)\n",
        "            d_sigmoid = sigmoid * (1.0 - sigmoid)\n",
        "            out[i] = (da_dx * (softsign - sigmoid) + a * d_softsign + one_minus_a * d_sigmoid)\n",
        "    return out\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def s4_cuda_device_function(x_val, derivative, k):\n",
        "    exp_neg_kx = math.exp(-k * x_val)\n",
        "    exp_neg_x = math.exp(-x_val)\n",
        "    a = 1.0 / (1.0 + exp_neg_kx)\n",
        "    if not derivative:\n",
        "        abs_x = math.fabs(x_val)\n",
        "        one_plus_abs_x = 1.0 + abs_x\n",
        "        softsign = x_val / one_plus_abs_x\n",
        "        sigmoid = 1.0 / (1.0 + exp_neg_x)\n",
        "        return a * softsign + (1.0 - a) * sigmoid\n",
        "    else:\n",
        "        one_minus_a = 1.0 - a\n",
        "        da_dx = k * a * one_minus_a\n",
        "        abs_x = math.fabs(x_val)\n",
        "        one_plus_abs_x = 1.0 + abs_x\n",
        "        softsign = x_val / one_plus_abs_x\n",
        "        d_softsign = 1.0 / (one_plus_abs_x * one_plus_abs_x)\n",
        "        sigmoid = 1.0 / (1.0 + exp_neg_x)\n",
        "        d_sigmoid = sigmoid * (1.0 - sigmoid)\n",
        "        return (da_dx * (softsign - sigmoid) + a * d_softsign + one_minus_a * d_sigmoid)\n",
        "\n",
        "@cuda.jit\n",
        "def s4_cuda_kernel(x_array, out_array, derivative, k):\n",
        "    idx = cuda.grid(1)\n",
        "    if idx < x_array.size:\n",
        "        out_array[idx] = s4_cuda_device_function(x_array[idx], derivative, k)\n",
        "\n",
        "def s4_accelerated(x: np.ndarray, derivative: bool = False, k: float = 5.0) -> np.ndarray:\n",
        "    if not isinstance(x, np.ndarray): x = np.array(x)\n",
        "    if GPU_AVAILABLE:\n",
        "        d_x = cuda.to_device(x)\n",
        "        d_out = cuda.device_array_like(d_x)\n",
        "        threads_per_block = 256\n",
        "        blocks_per_grid = (x.size + (threads_per_block - 1)) // threads_per_block\n",
        "        s4_cuda_kernel[blocks_per_grid, threads_per_block](d_x, d_out, derivative, k)\n",
        "        return d_out.copy_to_host()\n",
        "    else:\n",
        "        return s4_cpu_kernel(x, derivative, k)\n",
        "\n",
        "def s4_numpy(x: np.ndarray, derivative: bool = False, k: float = 5.0) -> np.ndarray:\n",
        "    exp_neg_kx = np.exp(-k * x)\n",
        "    exp_neg_x = np.exp(-x)\n",
        "    a = 1 / (1 + exp_neg_kx)\n",
        "    if not derivative:\n",
        "        abs_x = np.abs(x)\n",
        "        one_plus_abs_x = 1 + abs_x\n",
        "        softsign = x / one_plus_abs_x\n",
        "        sigmoid = 1 / (1 + exp_neg_x)\n",
        "        return a * softsign + (1 - a) * sigmoid\n",
        "    else:\n",
        "        one_minus_a = 1 - a\n",
        "        da_dx = k * a * one_minus_a\n",
        "        abs_x = np.abs(x)\n",
        "        one_plus_abs_x = 1 + abs_x\n",
        "        softsign = x / one_plus_abs_x\n",
        "        d_softsign = 1 / (one_plus_abs_x * one_plus_abs_x)\n",
        "        sigmoid = 1 / (1 + exp_neg_x)\n",
        "        d_sigmoid = sigmoid * (1.0 - sigmoid)\n",
        "        return (da_dx * (softsign - sigmoid) + a * d_softsign + one_minus_a * d_sigmoid)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    array_size = 10_000_000\n",
        "    test_data = np.linspace(-10, 10, array_size, dtype=np.float64)\n",
        "\n",
        "    if not GPU_AVAILABLE:\n",
        "        print(\"FATAL: GPU not available inside the Conda environment.\")\n",
        "    else:\n",
        "        print(\"CUDA GPU Available: True\")\n",
        "        exec_path = \"GPU\"\n",
        "        print(f\"Execution path for accelerated function: {exec_path}\\n\")\n",
        "\n",
        "        print(\"--- Correctness Check ---\")\n",
        "        numpy_result = s4_numpy(test_data)\n",
        "        accelerated_result = s4_accelerated(test_data)\n",
        "\n",
        "        if np.allclose(numpy_result, accelerated_result):\n",
        "            print(\"âœ… Correctness check PASSED. Outputs are numerically close.\")\n",
        "        else:\n",
        "            print(\"âŒ Correctness check FAILED.\")\n",
        "        print(\"-\" * 25, \"\\n\")\n",
        "\n",
        "        print(\"--- Performance Benchmark ---\")\n",
        "        number_of_runs = 100\n",
        "        accelerated_time = timeit.timeit('s4_accelerated(test_data)', globals=globals(), number=number_of_runs)\n",
        "        numpy_time = timeit.timeit('s4_numpy(test_data)', globals=globals(), number=number_of_runs)\n",
        "        print(f\"Original NumPy version took: {numpy_time:.6f} seconds\")\n",
        "        print(f\"Accelerated version ({exec_path}) took: {accelerated_time:.6f} seconds\")\n",
        "        if accelerated_time > 0:\n",
        "          speedup = numpy_time / accelerated_time\n",
        "          print(f\"\\nSpeedup: {speedup:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSNItr_cDZeH",
        "outputId": "3d620248-c1c8-4a24-b696-44618feec512"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda run -n my_env python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXFNqitXDdCg",
        "outputId": "d4a0f594-8605-4bdc-ebd6-061a0e52de3a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda/bin/ptxas\n",
            "ptxas: NVIDIA (R) Ptx optimizing assembler\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:14:54_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "--- Environment Verification ---\n",
            "Python version: 3.11.14 | packaged by conda-forge | (main, Oct 22 2025, 22:46:25) [GCC 14.3.0]\n",
            "Numba version: 0.62.1\n",
            "------------------------------\n",
            "\n",
            "CUDA GPU Available: True\n",
            "Execution path for accelerated function: GPU\n",
            "\n",
            "--- Correctness Check ---\n",
            "âœ… Correctness check PASSED. Outputs are numerically close.\n",
            "------------------------- \n",
            "\n",
            "--- Performance Benchmark ---\n",
            "Original NumPy version took: 37.019866 seconds\n",
            "Accelerated version (GPU) took: 6.001073 seconds\n",
            "\n",
            "Speedup: 6.17x\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# with separated file (library)"
      ],
      "metadata": {
        "id": "IIhseDviKxbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import numpy as np\n",
        "import timeit\n",
        "import sys\n",
        "\n",
        "# --- Import from the local s4.py file ---\n",
        "try:\n",
        "    from s4 import S4Activation, s4, s4_numpy\n",
        "except ImportError:\n",
        "    print(\"FATAL ERROR: Could not import from s4.py.\")\n",
        "    print(\"Please make sure 's4.py' is in the same directory as 'main.py'.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "def run_demo_and_benchmark():\n",
        "    \"\"\"Demonstrates the usage of the S4Activation module and runs a benchmark.\"\"\"\n",
        "    print(\"--- S4 Activation Function Demo & Benchmark ---\")\n",
        "\n",
        "    # --- 1. Demonstration of the S4Activation Class ---\n",
        "    print(\"\\n--- 1. Class-based API Demo ---\")\n",
        "\n",
        "    s4_auto = S4Activation(device='auto')\n",
        "\n",
        "    print(f\"Is GPU available according to the activator? {s4_auto.is_gpu_available}\")\n",
        "\n",
        "    small_array = np.array([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=np.float64)\n",
        "    print(f\"\\nInput: {small_array}\")\n",
        "    print(f\"s4(x): {s4_auto(small_array)}\")\n",
        "    print(f\"s4'(x): {s4_auto(small_array, derivative=True)}\")\n",
        "\n",
        "    large_array = np.ones(s4_auto.gpu_threshold + 1)\n",
        "    print(\"\\nBackend selection logic demo:\")\n",
        "    print(f\"Info for small array (size={small_array.size}): {s4_auto.get_backend_info(small_array)}\")\n",
        "    print(f\"Info for large array (size={large_array.size}): {s4_auto.get_backend_info(large_array)}\")\n",
        "\n",
        "    # --- 2. Demonstration of the s4() convenience function ---\n",
        "    print(\"\\n--- 2. Functional API Demo ---\")\n",
        "    y_cpu = s4(small_array, device='cpu')\n",
        "    print(f\"s4(x) with device='cpu' forced: {y_cpu}\")\n",
        "\n",
        "    # --- 3. Performance Benchmark ---\n",
        "    print(\"\\n--- 3. Performance Benchmark (on a very large array) ---\")\n",
        "\n",
        "    benchmark_data = np.linspace(-10, 10, 10_000_000, dtype=np.float64)\n",
        "\n",
        "    backend_info = s4_auto.get_backend_info(benchmark_data)\n",
        "    print(f\"Benchmarking with backend: '{backend_info['backend']}'\")\n",
        "\n",
        "    print(\"Performing a warm-up run (this includes compilation time)...\")\n",
        "    _ = s4_auto(benchmark_data)\n",
        "    print(\"Warm-up complete.\")\n",
        "\n",
        "    number_of_runs = 100\n",
        "\n",
        "    # --- CORRECTED PART ---\n",
        "    # Create a dictionary of the local variables that timeit needs to see.\n",
        "    timeit_namespace = {\n",
        "        \"s4_auto\": s4_auto,\n",
        "        \"benchmark_data\": benchmark_data,\n",
        "        \"s4_numpy\": s4_numpy  # Also include the numpy function for its benchmark\n",
        "    }\n",
        "\n",
        "    # Time the accelerated version using the custom namespace\n",
        "    accelerated_time = timeit.timeit(\n",
        "        's4_auto(benchmark_data)',\n",
        "        globals=timeit_namespace, # Pass the dictionary with our local variables\n",
        "        number=number_of_runs\n",
        "    )\n",
        "\n",
        "    # Time the pure NumPy version using the same namespace\n",
        "    numpy_time = timeit.timeit(\n",
        "        's4_numpy(benchmark_data)',\n",
        "        globals=timeit_namespace, # Pass the same dictionary\n",
        "        number=number_of_runs\n",
        "    )\n",
        "\n",
        "    print(f\"\\nBaseline (NumPy): {numpy_time:.4f} seconds for {number_of_runs} runs.\")\n",
        "    print(f\"Accelerated Path: {accelerated_time:.4f} seconds for {number_of_runs} runs.\")\n",
        "\n",
        "    if accelerated_time > 0 and 'numba' in sys.modules:\n",
        "        speedup = numpy_time / accelerated_time\n",
        "        print(f\"\\nSpeedup: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\n--- Benchmark Complete ---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_demo_and_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mIoJ6P5znOM",
        "outputId": "d3dd142f-585b-40eb-f36b-4804bf355c51"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda run -n my_env python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aim8q4AIznRi",
        "outputId": "c49338db-2d7b-4527-abed-ddc16076cbfc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- S4 Activation Function Demo & Benchmark ---\n",
            "\n",
            "--- 1. Class-based API Demo ---\n",
            "Is GPU available according to the activator? True\n",
            "\n",
            "Input: [-2. -1.  0.  1.  2.]\n",
            "s4(x): [0.11916725 0.26379501 0.25       0.50154644 0.66667639]\n",
            "s4'(x): [0.10481549 0.17140942 0.         0.24196223 0.11106223]\n",
            "\n",
            "Backend selection logic demo:\n",
            "Info for small array (size=5): {'backend': 'cpu', 'gpu_available': True, 'array_size': 5, 'array_shape': (5,), 'gpu_threshold': 10000, 'device_setting': 'auto'}\n",
            "Info for large array (size=10001): {'backend': 'gpu', 'gpu_available': True, 'array_size': 10001, 'array_shape': (10001,), 'gpu_threshold': 10000, 'device_setting': 'auto'}\n",
            "\n",
            "--- 2. Functional API Demo ---\n",
            "s4(x) with device='cpu' forced: [0.11916725 0.26379501 0.25       0.50154644 0.66667639]\n",
            "\n",
            "--- 3. Performance Benchmark (on a very large array) ---\n",
            "Benchmarking with backend: 'gpu'\n",
            "Performing a warm-up run (this includes compilation time)...\n",
            "Warm-up complete.\n",
            "\n",
            "Baseline (NumPy): 37.4321 seconds for 100 runs.\n",
            "Accelerated Path: 5.7903 seconds for 100 runs.\n",
            "\n",
            "Speedup: 6.46x\n",
            "\n",
            "--- Benchmark Complete ---\n",
            "\n"
          ]
        }
      ]
    }
  ]
}